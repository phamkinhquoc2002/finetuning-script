dataset:
  path: nvidia/OpenMathReasoning
  format: standard
training_config:
  model: ./my_saved_model
  trainer: SFT
  distributed_training: Unsloth
  per_device_train_batch_size: 2
  gradient_acc_steps: 1
  optim: adamw_torch_fused
  learning_rate: 5.0e-05
  num_train_epochs: 2
  lr_scheduler_type: cosine
  save_steps: 100
  max_completion_length: 2048
  max_seq_length: 4096
  gradient_checkpointing: true
  report_to: tensorboard
