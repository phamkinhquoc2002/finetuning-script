model: ./my_saved_model/qwen3-8b
trainer: SFT
distributed_training: FSDP
per_device_train_batch_size: 1
gradient_acc_steps: 4
optim: adamw_torch_fused
learning_rate: 5e-5
num_train_epochs: 1
lr_scheduler_type: cosine
save_steps: 50
max_completion_length: 1024
max_seq_length: 2048
gradient_checkpointing: False
report_to: tensorboard